{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take the input train data\n",
    "\n",
    "2. Devide them into mini batches\n",
    "\n",
    "3. Iterate over the mini_batchs\n",
    "    \n",
    "    a. Take a mini batch \n",
    "    \n",
    "    b. iterate over the mini batch \n",
    "    \n",
    "    c. run the feed forward algorithm \n",
    "    \n",
    "    d. calculate the delta_bias and delta_weight \n",
    "       i.e. calculate the change in cost function w.r.t. change in bias and weight respectively\n",
    "    \n",
    "    e. don't back propagate for each input, just return the delta_bias, delta_w\n",
    "    \n",
    "    f. after finishing the mini batch iteration, tweak the biases and weights using the following formulas:\n",
    "    \n",
    "    ratio = eta / size_of_mini_batch\n",
    "    \n",
    "    w -> w' = w -  ratio * sum_of_all_delta_w\n",
    "    \n",
    "    b -> b' = b - ratio * sum_of_all_delta_b   \n",
    "    \n",
    "    g. After tweaking the changes dont run the feedforward again\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    run the back propagation one time but over what ?\n",
    "    \n",
    "    he he he ! over the each of the input of the mini_batch \n",
    "    \n",
    "    again do the same process\n",
    "    \n",
    "    \n",
    "    Question :  \n",
    "    When to stop  ?\n",
    "    \n",
    "    Ans :  1. stop when all the mini_batch outputs are correct \n",
    "           2. stop when the average cost error is less may be less then 0.2 or something else\n",
    "           \n",
    "           \n",
    "           \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import random\n",
    "\n",
    "from scipy import misc \n",
    "import imageio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    result = (x - (x**2))\n",
    "    #print(\"\\n Sigmoid derivative is : \", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function split the array by choosing the size random indexes\n",
    "#size of the new array\n",
    "# return two arrays \n",
    "def splitArray(arr, size):\n",
    "    \n",
    "    #systemRandom = random.SystemRandom()\n",
    "    #_index = np.array ( [ systemRandom.randint(0, len(arr)-1) for index in range(size) ] )\n",
    "\n",
    "    _index = np.array ( [ index for index in random.sample(range(0,len(arr)), size)] )\n",
    "    \n",
    "    # choose the index values from the array\n",
    "    new_arr = arr.take(_index)\n",
    "    arr = np.delete(arr, _index)\n",
    "    return arr, new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(img):\n",
    "    plt.imshow(img, \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self, layers_size, learning_rate, output_size):\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.layers_size = layers_size\n",
    "        self.total_layers = len(layers_size)\n",
    "        self.activation_layer = [] \n",
    "        self.eta = learning_rate\n",
    "        self.thershold = 0.95\n",
    "        \n",
    "        # remember total number of rows are equal to the number of neurons in the next layer \n",
    "        self.weights = [  np.random.randn(row,col) for row, col in zip(self.layers_size[1:], self.layers_size[:-1]) ] \n",
    "        \n",
    "        # 1-d vector\n",
    "        self.biases =  [ np.random.randn(row, 1) for row in self.layers_size[1:] ]\n",
    "        \n",
    "    \n",
    "    # x -> input layer\n",
    "    def feed_forward(self, x): \n",
    "        \n",
    "        self.activation_layer = [x]\n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            \n",
    "            # w x  + b\n",
    "            next_layer = sigmoid (np.dot(w, x) + b) \n",
    "            \n",
    "            # storing the layerwise activation for using in the cost error calculations\n",
    "            self.activation_layer.append(next_layer)\n",
    "            \n",
    "            x = next_layer \n",
    "            \n",
    "#------------------------------------------------------------------------------------------------------------------------            \n",
    "            \n",
    "        \n",
    "    # ( activation_layer[L] - Y(x) ) *  (sigmoid(z) - (sigmoid(z))^2)\n",
    "    # ( Al - Y(x) ) * ( sigmoid(z) - (sigmoid(z)^2))\n",
    "    def get_output_layer_error(self, expected_output):\n",
    "\n",
    "        # since the layers variable contains the activation neuron for each layer \n",
    "        return (self.activation_layer[-1] - expected_output) * sigmoid_derivative(self.activation_layer[-1])\n",
    "\n",
    "    \n",
    "    # get the layer wise delta_l means layer wise error \n",
    "    # (transpose(weight[l+1])*delta_l(l+1)) * ( sigmoid(z) - (sigmoid(z)^2))\n",
    "    def get_layerwise_error(self, expected_output):\n",
    "    \n",
    "        delta_l = [self.get_output_layer_error(expected_output)]\n",
    "\n",
    "        \n",
    "        #print(\"\\nShape of output layer : \", np.shape(delta_l[0]))\n",
    "        # suppose there are 3 layers suppose there are n layers \n",
    "        # according to our code      according to our code \n",
    "        # 0 to 1 are the weights   0 to n-2 are the weights        \n",
    "        # so starting from 3-2 = 1   so starting from n-2 = 1 \n",
    "        # delta_l contains layer + 1 error at starting index -> 0 \n",
    "        \n",
    "        for layer in range(self.total_layers-2, 0 ,-1):\n",
    "            \n",
    "            err = np.dot(np.transpose(self.weights[layer]), delta_l[0] )* sigmoid_derivative(self.activation_layer[layer])\n",
    "            \n",
    "            #print(\"\\n err shape : \", np.shape(err))\n",
    "            # insert error at the beginning\n",
    "            delta_l.insert(0, err)\n",
    "            \n",
    "        return delta_l\n",
    "\n",
    "            \n",
    "    # this function is for getting the error\n",
    "    def back_propagation(self, data, expected_output):\n",
    "    \n",
    "        delta_l = self.get_layerwise_error(expected_output)\n",
    "    \n",
    "        # calculate the change in weight and bias \n",
    "        \n",
    "        # change in cost if there is a change in the bias\n",
    "        delta_bias = delta_l\n",
    "    \n",
    "        delta_weight = [ np.zeros(np.shape(w)) for w in self.weights]\n",
    "        \n",
    "        # we have the delta_layers calculated, we have activation_layers then \n",
    "        # we can start from 0 to calculate the weight error\n",
    "        # it is not a backward paas but we can take the advantage of already calculated values \n",
    "        \n",
    "        delta_weight = [delta_weight[l]+np.dot(delta_l[l], np.transpose(self.activation_layer[l])) for l in range(self.total_layers-1)  ] \n",
    "    \n",
    "        #print(\"\\n Delta Weight Shape : \", np.shape(delta_weight[0]))\n",
    "        # find the alternative way to push in front \n",
    "    \n",
    "        #print(\"\\n before return shape delta_weight:  \", np.shape(delta_weight))\n",
    "        return delta_bias, delta_weight\n",
    "    \n",
    "\n",
    "    # this function tweak the weights and biases of the network \n",
    "    def update_weight_bias(self, mini_batches):\n",
    "\n",
    "\n",
    "        #print(\"\\n Weights Initialy : \\n\", self.weights)\n",
    "        #print(\"\\n Biases Initialy : \\n\", self.biases)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #m = 1 \n",
    "        # iterate over the mini batch \n",
    "        for mini_batch in mini_batches :\n",
    "            \n",
    "            avg_delta_bias = [ np.zeros(np.shape(b)) for b in self.biases ]\n",
    "            avg_delta_weight = [ np.zeros(np.shape(w)) for w in self.weights ]\n",
    "         \n",
    "            \n",
    "            # c = 0\n",
    "            mini_batch_size = len(mini_batch)\n",
    "            #print(\"\\n mini batch size : \", mini_batch_size)\n",
    "            # iterate over the inputs of a mini_batch \n",
    "            for data, label in mini_batch :\n",
    "\n",
    "                self.feed_forward(data)\n",
    "                \n",
    "                #print(\"\\n Feed forward : \\n\", self.activation_layer[-1])\n",
    "            \n",
    "                \n",
    "                # get the change in bias and weight \n",
    "                # get the error \n",
    "                # either send the input as one hot vector or send the label as integer\n",
    "                expected_output = np.zeros(shape=(self.output_size,1)) # shape chaning for testing \n",
    "                expected_output[label] = 1 \n",
    "                #print(\"\\n Expected output : \", expected_output)\n",
    "                #print(\"\\n Actual output : \", self.activation_layer[-1])\n",
    "                \n",
    "                delta_bias, delta_weight = self.back_propagation(data, expected_output)\n",
    "\n",
    "                #print(\"\\n Delta bias : \\n\", delta_bias)\n",
    "                #print(\"\\n Delta Weight : \\n\", delta_weight)\n",
    "                \n",
    "                \n",
    "                #print(\"\\navg : \",np.shape(avg_delta_weight))\n",
    "                \n",
    "                #print(\"\\nnrml : \",np.shape(delta_weight))\n",
    "                \n",
    "                # remember the error biases and weights are also in the same dimension as avg one\n",
    "                avg_delta_bias = np.add(avg_delta_bias, delta_bias)\n",
    "                \n",
    "                \n",
    "                #print(\"\\n avg_delta_weight : \",avg_delta_weight)\n",
    "                #print(\"\\n delta_weight : \",type(delta_weight))\n",
    "                \n",
    "                avg_delta_weight = [ np.add(avg_del_w, del_w) for avg_del_w, del_w in zip(avg_delta_weight, delta_weight) ]\n",
    "                \n",
    "                #print(\"\\n Avg Delta bias : \\n\", delta_bias)\n",
    "                #print(\"\\n Avg Delta Weight : \\n\", delta_weight)\n",
    "            \n",
    "            #m += 1\n",
    "            # now remove the average error from the weight and bias\n",
    "            self.weights = [ weight - (self.eta/mini_batch_size) * avg_weight for weight, avg_weight in zip(self.weights, avg_delta_weight)] \n",
    "            self.biases = [ bias - (self.eta/mini_batch_size) * avg_bias for bias, avg_bias in zip(self.biases, avg_delta_bias)] \n",
    "    \n",
    "            #print(\"\\n Weights after \\n\", self.weights)\n",
    "            #print(\"\\n Biases after \\n\", self.biases)\n",
    "            \n",
    "            #print(\"\\nlength of weights : \", len(self.weights))\n",
    "\n",
    "    # this function is for training the data \n",
    "    def train_network(self, training_data, testing_data, mini_batch_size, ephoc) :\n",
    "\n",
    "\n",
    "        for i in range(ephoc):\n",
    "            \n",
    "            print(\"\\n Running for ephoc \", i)\n",
    "            random.shuffle(training_data)\n",
    "\n",
    "            # get the mini batches in the network \n",
    "            mini_batches = [ training_data[k:k+mini_batch_size] for k in range(0, len(training_data) , mini_batch_size)  ]\n",
    "\n",
    "            self.update_weight_bias(mini_batches)\n",
    "            self.test_network(testing_data)\n",
    "        \n",
    "        #print(\"Activation layer at last is : \", self.activation_layer[self.total_layers-1])\n",
    "        \n",
    "        \n",
    "\n",
    "    # this is for the testing data\n",
    "    def test_network(self, testing_data):\n",
    "\n",
    "        count = 0\n",
    "        for data, label in testing_data:\n",
    "\n",
    "            self.feed_forward(data)\n",
    "\n",
    "            output_layer = self.activation_layer[self.total_layers-1]\n",
    "            \n",
    "            v = [ np.where(output_layer > self.thershold) ] \n",
    "\n",
    "            #print(\"Value : \", v)\n",
    "            if len(v[0][0]) == 1 and v[0][0] == label:\n",
    "                count += 1\n",
    "        \n",
    "        print (\"Ans is {0} / {1}\".format(count ,len(testing_data)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the data either in tuple form or in the x_train, y_train both should be in the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784, 1)\n"
     ]
    }
   ],
   "source": [
    "# read the input\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "shape = np.shape(x_train) \n",
    "\n",
    "#reshape the trained data into single vector \n",
    "new_x_train = np.reshape(x_train, newshape=( len(x_train), shape[1]*shape[2], 1) )\n",
    "\n",
    "print(np.shape(new_x_train))\n",
    "\n",
    "total_testing_data = len(x_test)  # 10000\n",
    "total_training_data = len(x_train) - total_testing_data # 60000 - 10000\n",
    "\n",
    "# call the feed forward to get the output layer \n",
    "#output_layer = feed_forward(hdr_nn, input_layer, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# normalize the data to make each value between 0 and 1\n",
    "new_x_train = new_x_train/255\n",
    "\n",
    "# split the data \n",
    "# 50000 and 10000\n",
    "\n",
    "new_x_test = new_x_train[total_training_data:]\n",
    "print(len(new_x_test))\n",
    "\n",
    "new_y_test = y_train[total_training_data:]\n",
    "print(len(new_y_test))\n",
    "\n",
    "new_x_train = new_x_train[:total_training_data]\n",
    "print(len(new_x_train))\n",
    "\n",
    "new_y_train = y_train[:total_training_data]\n",
    "print(len(new_y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the numpy array to list of tuples\n",
    "\n",
    "training_data = [ (data, label) for data, label in zip(new_x_train, new_y_train)]\n",
    "len(training_data)\n",
    "\n",
    "testing_data = [ (data, label) for data, label in zip(new_x_test, new_y_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running for ephoc  0\n",
      "Ans is 3826 / 10000\n",
      "\n",
      " Running for ephoc  1\n",
      "Ans is 4832 / 10000\n",
      "\n",
      " Running for ephoc  2\n",
      "Ans is 6087 / 10000\n",
      "\n",
      " Running for ephoc  3\n",
      "Ans is 6412 / 10000\n",
      "\n",
      " Running for ephoc  4\n",
      "Ans is 6801 / 10000\n",
      "\n",
      " Running for ephoc  5\n",
      "Ans is 6848 / 10000\n",
      "\n",
      " Running for ephoc  6\n",
      "Ans is 7252 / 10000\n",
      "\n",
      " Running for ephoc  7\n",
      "Ans is 7363 / 10000\n",
      "\n",
      " Running for ephoc  8\n",
      "Ans is 7547 / 10000\n",
      "\n",
      " Running for ephoc  9\n",
      "Ans is 7539 / 10000\n",
      "\n",
      " Running for ephoc  10\n",
      "Ans is 7681 / 10000\n",
      "\n",
      " Running for ephoc  11\n",
      "Ans is 7491 / 10000\n",
      "\n",
      " Running for ephoc  12\n",
      "Ans is 7701 / 10000\n",
      "\n",
      " Running for ephoc  13\n",
      "Ans is 7787 / 10000\n",
      "\n",
      " Running for ephoc  14\n",
      "Ans is 7885 / 10000\n",
      "\n",
      " Running for ephoc  15\n",
      "Ans is 7882 / 10000\n",
      "\n",
      " Running for ephoc  16\n",
      "Ans is 7919 / 10000\n",
      "\n",
      " Running for ephoc  17\n",
      "Ans is 7973 / 10000\n",
      "\n",
      " Running for ephoc  18\n",
      "Ans is 8070 / 10000\n",
      "\n",
      " Running for ephoc  19\n",
      "Ans is 8148 / 10000\n"
     ]
    }
   ],
   "source": [
    "not_trng_data = training_data[:50000]\n",
    "\n",
    "not_testing_data = testing_data[:10000]\n",
    "\n",
    "#number of neurons in layer\n",
    "layer_wise_neuron_count = [784, 30, 10]\n",
    "\n",
    "# create object for neural network\n",
    "hdr_nn = Network(layer_wise_neuron_count, 1.0, 10)\n",
    "\n",
    "hdr_nn.train_network(not_trng_data, not_testing_data, mini_batch_size=10, ephoc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ans is 8831 / 10000\n"
     ]
    }
   ],
   "source": [
    "shape = np.shape(x_test)\n",
    "\n",
    "#reshape the trained data into single vector \n",
    "real_testing_data = np.reshape(x_test, newshape=( len(x_test), shape[1]*shape[2], 1) )\n",
    "\n",
    "real_testing_data = real_testing_data /255\n",
    "\n",
    "real_testing_data = [(data, label) for data, label in zip(real_testing_data, y_test)]\n",
    "\n",
    "hdr_nn.test_network(real_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running for ephoc  0\n",
      "Ans is 8154 / 10000\n",
      "\n",
      " Running for ephoc  1\n",
      "Ans is 8129 / 10000\n",
      "\n",
      " Running for ephoc  2\n",
      "Ans is 8143 / 10000\n",
      "\n",
      " Running for ephoc  3\n",
      "Ans is 8256 / 10000\n",
      "\n",
      " Running for ephoc  4\n",
      "Ans is 8288 / 10000\n",
      "\n",
      " Running for ephoc  5\n",
      "Ans is 8205 / 10000\n",
      "\n",
      " Running for ephoc  6\n",
      "Ans is 8290 / 10000\n",
      "\n",
      " Running for ephoc  7\n",
      "Ans is 8205 / 10000\n",
      "\n",
      " Running for ephoc  8\n",
      "Ans is 8246 / 10000\n",
      "\n",
      " Running for ephoc  9\n",
      "Ans is 8285 / 10000\n",
      "\n",
      " Running for ephoc  10\n",
      "Ans is 8321 / 10000\n",
      "\n",
      " Running for ephoc  11\n",
      "Ans is 8407 / 10000\n",
      "\n",
      " Running for ephoc  12\n",
      "Ans is 8398 / 10000\n",
      "\n",
      " Running for ephoc  13\n",
      "Ans is 8346 / 10000\n",
      "\n",
      " Running for ephoc  14\n",
      "Ans is 8347 / 10000\n",
      "\n",
      " Running for ephoc  15\n",
      "Ans is 8362 / 10000\n",
      "\n",
      " Running for ephoc  16\n",
      "Ans is 8451 / 10000\n",
      "\n",
      " Running for ephoc  17\n",
      "Ans is 8466 / 10000\n",
      "\n",
      " Running for ephoc  18\n",
      "Ans is 8449 / 10000\n",
      "\n",
      " Running for ephoc  19\n",
      "Ans is 8495 / 10000\n"
     ]
    }
   ],
   "source": [
    "hdr_nn.train_network(not_trng_data, not_testing_data, mini_batch_size=10, ephoc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running for ephoc  0\n",
      "\n",
      " layer :  1\n",
      "\n",
      " layer :  1\n",
      "Ans is 0 / 2\n",
      "\n",
      " Running for ephoc  1\n",
      "\n",
      " layer :  1\n",
      "\n",
      " layer :  1\n",
      "Ans is 0 / 2\n",
      "\n",
      " Running for ephoc  2\n",
      "\n",
      " layer :  1\n",
      "\n",
      " layer :  1\n",
      "Ans is 0 / 2\n"
     ]
    }
   ],
   "source": [
    "# testing the input against the small input\n",
    "\n",
    "# 0 or 1 is the output value\n",
    "train = [ (np.array([[0.4], [0.5], [0.6]]), 1), (np.array([[0.3], [0.8], [0.5]]), 0) ] \n",
    "\n",
    "#number of neurons in layer\n",
    "layer_wise_neuron_count = [3, 2, 2]\n",
    "\n",
    "# create object for neural network\n",
    "hdr_nn = Network(layer_wise_neuron_count, 0.1)\n",
    "\n",
    "hdr_nn.train_network(train, train, mini_batch_size=1, ephoc=3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
